// Copyright 2021 by David A. Golden. All rights reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License"); you may
// not use this file except in compliance with the License. You may obtain
// a copy of the License at http://www.apache.org/licenses/LICENSE-2.0

// Package strum provides a string unmarshaler to convert line-oriented text
// (such as from STDIN) into simple Go structs.  Fields may be of primitive
// types:
//
//  - strings
//  - booleans ('true', 'false'; case insensitive)
//  - integers (signed and unsigned)
//
// For integers, all Go integer literal formats are supported, including
// base prefixes (`0xff`) and underscores (`1_000_000`).
//
// Additionally, there is special support for certain types:
//
//  - time.Time (only RFC 3339 strings supported at the moment)
//
// Field extraction defaults to whitespace-separated fields, but strum
// supports using delimiters, regular expressions, or a custom tokenizer.
package strum

import (
	"bufio"
	"errors"
	"fmt"
	"io"
	"reflect"
	"regexp"
	"strconv"
	"strings"
	"time"
)

// A Tokenizer is a function that breaks an input string into tokens.
type Tokenizer func(s string) ([]string, error)

// A Decoder converts an input stream into structs.
type Decoder struct {
	s *bufio.Scanner
	t Tokenizer
}

// NewDecoder returns a Decoder that reads from r. The default Decoder will
// tokenize with `strings.Fields` function.
func NewDecoder(r io.Reader) *Decoder {
	return &Decoder{
		s: bufio.NewScanner(r),
		t: func(s string) ([]string, error) { return strings.Fields(s), nil },
	}
}

// WithTokenizer modifies a Decoder to use a customer tokenizing function.
func (d *Decoder) WithTokenizer(t Tokenizer) *Decoder {
	d.t = t
	return d
}

// WithTokenRegexp modifies a Decoder to use a regular expression to extract
// string fields.  The regular expression is called with `FindStringSubmatches`
// for each line of input, so it must encompass an entire line of input.
func (d *Decoder) WithTokenRegexp(re *regexp.Regexp) *Decoder {
	return d.WithTokenizer(
		func(s string) ([]string, error) {
			xs := re.FindStringSubmatch(s)
			if xs == nil {
				return []string{}, errors.New("regexp failed to match line " + s)
			}
			// If the regexp had no submatches, then there are no tokens.
			// XXX Should this be an error?
			if len(xs) == 1 {
				return []string{}, nil
			}
			// Drop the full match and return only submatches.
			return xs[1:], nil
		},
	)
}

// WithSplitOn modifies a Decoder to split fields on a separator string.
func (d *Decoder) WithSplitOn(sep string) *Decoder {
	return d.WithTokenizer(
		func(s string) ([]string, error) {
			return strings.Split(s, sep), nil
		},
	)
}

// Tokens returns all strings generated by the tokenizer.  It is used
// internally by `Decode`, but made available for testing and diagnostics.
func (d *Decoder) Tokens() ([]string, error) {
	s, err := d.readline()
	if err != nil {
		return nil, err
	}
	return d.t(s)
}

func (d *Decoder) readline() (string, error) {
	if !(d.s.Scan()) {
		err := d.s.Err()
		if err != nil {
			return "", err
		}
		return "", io.EOF
	}
	return d.s.Text(), nil
}

// Decode reads the next line of input, tokenizes it, and converts tokens
// into the fields of `v` in order.  The `v` argument must be a pointer to
// a struct.  If the input has fewer tokens than fields in the struct, the
// extra fields will be left with zero values.
func (d *Decoder) Decode(v interface{}) error {
	argValue := reflect.ValueOf(v)

	// XXX What if Nil?
	if argValue.Kind() != reflect.Ptr {
		return fmt.Errorf("argument to Decode must be a pointer, not %s", argValue.Kind())
	}

	destValue := argValue.Elem()
	if destValue.Kind() != reflect.Struct {
		return fmt.Errorf("argument to Decode must be a pointer to struct, not %s", destValue.Kind())
	}

	tokens, err := d.Tokens()
	if err != nil {
		return err
	}

	destType := destValue.Type()
	destNS := destType.PkgPath() + "." + destType.Name()

	// map tokens into v
	numFields := destValue.NumField()
	for i := range tokens {
		if i >= numFields {
			break
		}
		fieldName := destNS + "." + destType.Field(i).Name
		err = decodeToField(fieldName, destValue.Field(i), tokens[i])
		if err != nil {
			return err
		}
	}

	return nil
}

func decodingError(name string, err error) error {
	return fmt.Errorf("error decoding to field '%s': %w", name, err)
}

var timeType = reflect.TypeOf(time.Time{})

func decodeToField(name string, v reflect.Value, s string) error {
	// XXX here if it can TextUnmarshal, if so, do that.  But maybe do after
	// special casing for types?  I.e. strum special casing overrides
	// TextUnmarshal?

	// Custom parsing for certain types
	switch v.Type() {
	case timeType:
		t, err := time.Parse(time.RFC3339, s)
		if err != nil {
			return decodingError(name, err)
		}
		v.Set(reflect.ValueOf(t))
		return nil
	}

	switch v.Kind() {
	case reflect.Bool:
		switch strings.ToLower(s) {
		case "true":
			v.SetBool(true)
		case "false":
			v.SetBool(false)
		default:
			return decodingError(name, fmt.Errorf("error decoding '%s' as boolean", s))
		}
	case reflect.String:
		v.SetString(s)
	case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64:
		i, err := strconv.ParseInt(s, 0, v.Type().Bits())
		if err != nil {
			return decodingError(name, err)
		}
		v.SetInt(i)
	case reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64:
		i, err := strconv.ParseUint(s, 0, v.Type().Bits())
		if err != nil {
			return decodingError(name, err)
		}
		v.SetUint(i)
	case reflect.Float32, reflect.Float64:
		return decodingError(name, errors.New("float not yet supported"))
	default:
		return decodingError(name, errors.New("unsupported type"))
	}

	return nil
}
